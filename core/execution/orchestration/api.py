"""
Execution API - Core Data Models

Defines the contract for execution orchestration.
These models are framework-agnostic and used across all execution strategies.
"""

from dataclasses import dataclass, field
from datetime import datetime
from enum import Enum
from typing import Dict, List, Optional, Any


class StrategyType(str, Enum):
    """Execution strategy types"""
    SMOKE = "smoke"
    IMPACTED = "impacted"
    RISK_BASED = "risk"
    FULL = "full"


class ExecutionStatus(str, Enum):
    """Execution status"""
    PENDING = "pending"
    PLANNING = "planning"
    EXECUTING = "executing"
    COMPLETED = "completed"
    FAILED = "failed"
    CANCELLED = "cancelled"


@dataclass
class ExecutionRequest:
    """
    Request to execute tests.
    
    This is the entry point for all test execution orchestration.
    Crossbridge uses this to determine what to run and how.
    """
    framework: str  # testng | robot | pytest | cypress | playwright | cucumber | etc.
    strategy: StrategyType  # How to select tests
    environment: str  # dev | qa | staging | prod
    ci_mode: bool = False  # Affects logging, retries, parallelism
    dry_run: bool = False  # Plan only, no actual execution
    metadata: Dict[str, str] = field(default_factory=dict)  # branch, commit, build_id
    
    # Optional constraints
    max_tests: Optional[int] = None  # Budget limit
    max_duration_minutes: Optional[int] = None  # Time budget
    tags: Optional[List[str]] = None  # Filter by tags
    exclude_tags: Optional[List[str]] = None  # Exclude tags
    include_flaky: bool = False  # Whether to include known flaky tests
    parallel: bool = True  # Enable parallel execution
    
    # Git context (for impacted strategy)
    base_branch: Optional[str] = None
    changed_files: Optional[List[str]] = None
    
    def __post_init__(self):
        """Validate request"""
        if isinstance(self.strategy, str):
            self.strategy = StrategyType(self.strategy)


@dataclass
class ExecutionPlan:
    """
    Plan for test execution.
    
    Generated by execution strategies, consumed by framework adapters.
    This is the bridge between "what to run" and "how to invoke the framework".
    """
    selected_tests: List[str]  # Test identifiers (class names, file paths, etc.)
    skipped_tests: List[str]  # Tests not selected
    grouping: Dict[str, List[str]]  # Groups of tests (for parallel execution)
    priority: Dict[str, int]  # Test priorities (1-5, 5 = highest)
    reasons: Dict[str, str]  # Why each test was selected/skipped
    
    # Execution settings
    framework: str
    strategy: StrategyType
    environment: str
    parallel: bool
    max_duration_minutes: Optional[int] = None
    
    # Metadata
    created_at: datetime = field(default_factory=datetime.utcnow)
    estimated_duration_minutes: Optional[int] = None
    confidence_score: float = 1.0  # How confident we are in this plan (0-1)
    
    def total_tests(self) -> int:
        """Total number of tests selected"""
        return len(self.selected_tests)
    
    def reduction_percentage(self, total_available: int) -> float:
        """Percentage reduction from full suite"""
        if total_available == 0:
            return 0.0
        return (1 - len(self.selected_tests) / total_available) * 100


@dataclass
class ExecutionResult:
    """
    Result of test execution.
    
    Standardized result format regardless of framework.
    Parsed from framework-specific reports.
    """
    executed_tests: List[str]  # Tests that actually ran
    passed_tests: List[str]  # Successful tests
    failed_tests: List[str]  # Failed tests
    skipped_tests: List[str]  # Skipped during execution
    error_tests: List[str]  # Tests with errors (not failures)
    
    # Execution metrics
    execution_time_seconds: float
    start_time: datetime
    end_time: datetime
    
    # Framework artifacts
    report_paths: List[str]  # Paths to framework reports
    log_paths: List[str]  # Paths to logs
    framework: str
    environment: str
    
    # Metadata
    build_id: Optional[str] = None
    commit_sha: Optional[str] = None
    branch: Optional[str] = None
    
    # Status
    status: ExecutionStatus = ExecutionStatus.COMPLETED
    error_message: Optional[str] = None
    
    def pass_rate(self) -> float:
        """Calculate pass rate"""
        total = len(self.executed_tests)
        if total == 0:
            return 0.0
        return len(self.passed_tests) / total * 100
    
    def failure_rate(self) -> float:
        """Calculate failure rate"""
        return 100 - self.pass_rate()
    
    def has_failures(self) -> bool:
        """Check if any tests failed"""
        return len(self.failed_tests) > 0 or len(self.error_tests) > 0


@dataclass
class ExecutionContext:
    """
    Execution context for strategies.
    
    Provides all the information needed for intelligent test selection.
    Aggregates data from git, memory, results, and other sources.
    """
    # Request
    request: ExecutionRequest
    
    # Available tests
    available_tests: List[str]  # All tests that could be run
    test_metadata: Dict[str, Dict[str, Any]]  # Test metadata (tags, priority, etc.)
    
    # Git context (for impacted strategy)
    changed_files: List[str] = field(default_factory=list)
    base_commit: Optional[str] = None
    target_commit: Optional[str] = None
    
    # Historical data (for risk-based strategy)
    test_history: Dict[str, List[Dict[str, Any]]] = field(default_factory=dict)
    flaky_tests: List[str] = field(default_factory=list)
    failure_rates: Dict[str, float] = field(default_factory=dict)
    
    # Coverage data (for impacted strategy)
    test_to_code_mapping: Dict[str, List[str]] = field(default_factory=dict)
    code_to_test_mapping: Dict[str, List[str]] = field(default_factory=dict)
    
    # Crossbridge memory (for AI-powered strategies)
    memory_graph: Optional[Any] = None  # Graph of test relationships
    
    # Environment
    ci_environment: bool = False
    available_resources: Dict[str, Any] = field(default_factory=dict)
    
    def get_test_priority(self, test_id: str) -> int:
        """Get priority for a test (1-5, default 3)"""
        metadata = self.test_metadata.get(test_id, {})
        return metadata.get("priority", 3)
    
    def get_test_tags(self, test_id: str) -> List[str]:
        """Get tags for a test"""
        metadata = self.test_metadata.get(test_id, {})
        return metadata.get("tags", [])
    
    def is_flaky(self, test_id: str) -> bool:
        """Check if test is known to be flaky"""
        return test_id in self.flaky_tests
    
    def get_failure_rate(self, test_id: str) -> float:
        """Get historical failure rate for a test (0-1)"""
        return self.failure_rates.get(test_id, 0.0)
    
    def is_impacted(self, test_id: str) -> bool:
        """Check if test is impacted by changed files"""
        if not self.changed_files:
            return False
        
        covered_files = self.test_to_code_mapping.get(test_id, [])
        return any(changed in covered_files for changed in self.changed_files)
